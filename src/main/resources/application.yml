server:
  port: ${SPRING_LOCAL_PORT:8080}

spring:
  application:
    name: inventory-management-service

  config:
    import: optional:file:.env[.properties]

  datasource:
    hikari:
      connection-timeout: 10000
      maximum-pool-size: ${HIKARI_MAXIMUM_POOL_SIZE:10}
      minimum-idle: 1
      idle-timeout: 10000
    type: com.zaxxer.hikari.HikariDataSource
    driverClassName: org.postgresql.Driver
    url: jdbc:postgresql://${DB_HOST:localhost}/${POSTGRES_DB:postgres}?currentSchema=inventory_management_service
    username: ${POSTGRES_USER:postgres}
    password: ${POSTGRES_PASSWORD:postgres}

  jpa:
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    hibernate:
      ddl-auto: validate
    show-sql: false

  flyway:
    enabled: true
    baseline-version: 0
    baseline-on-migrate: true
    table: schema_version
    validate-on-migrate: false

  data:
    redis:
      host: ${REDIS_HOST:localhost}
      port: ${REDIS_PORT:6379}
      password:
      jedis:
        pool:
          max-active: 7
          max-idle: 7
          min-idle: 2
          max-wait: -1ms

  jackson:
    default-property-inclusion: non_null
    serialization:
      indent-output: true

  threads:
    virtual:
      enabled: true

  docker:
    compose:
      enabled: true

  cloud:
    function:
      definition: internalEventProducer;internalEventConsumer;userCreatedEvent;

    stream:
      bindings:
        internalEventProducer-in-0:
          destination: internal-event-destination
        internalEventProducer-out-0:
          destination: internal-event-processed-destination
        internalEventConsumer-in-0:
          destination: internal-event-processed-destination
        userCreatedEvent-in-0:
          destination: user-created-event-destination
        userCreatedEvent-out-0:
          destination: user-created-event-destination
      kafka:
        binder:
          brokers: ${KAFKA_BROKERS:localhost:9092}

  kafka:
    consumer:
      bootstrap-servers: ${KAFKA_CONSUMER_BOOTSTRAP_SERVERS:localhost:9092}
      group-id: mb
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring:
          json:
            trusted:
              packages: '*'
    producer:
      bootstrap-servers: ${KAFKA_PRODUCER_BOOTSTRAP_SERVERS:localhost:9092}
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer

management:
  metrics:
    distribution:
      percentiles-histogram:
        http:
          server:
            requests: true
  tracing:
    sampling:
      probability: 1.0
  endpoints:
    web:
      exposure:
        include: "health,metrics,prometheus"
  endpoint:
    health:
      show-details: always
  newrelic:
    metrics:
      export:
        api-key: ${NEW_RELIC_LICENSE_KEY}
        endpoint: ${NEW_RELIC_ENDPOINT}
        enabled: ${NEW_RELIC_ENABLED}
  zipkin:
    tracing:
      endpoint: http://${ZIPKIN_HOST:localhost}:9411/api/v2/spans

springdoc:
  api-docs:
    path: /api-docs
  swagger-ui:
    path: /swagger-ui.html
    enabled: true

# traceID and spanId are predefined MDC keys - we want the logs to include them
logging:
  pattern:
    level: "%5p [${spring.application.name},%X{traceId:-},%X{spanId:-}]"

testing:
  app:
    jwt-secret: testingSecretKeyTestingSecretKeyTestingSecretKeyTestingSecretKeyTestingSecretKeyTestingSecretKey
    jwt-expiration-ms: 600000 # token will expire in 10 minutes

token:
  store: redis
